# HEADER
# -----------------------------------------------------------------------------	

BootStrap: docker
From: nvidia/cuda:10.1-devel-ubuntu18.04

# SECTIONS
# -----------------------------------------------------------------------------

%setup

# During the build process, commands in the %setup section are first executed 
# on the host system outside of the container after the base OS has been installed. 
# You can reference the container file system with the $SINGULARITY_ROOTFS environment
# variable in the %setup section. This scriptlet is executed outside of the container
# on the host system itself, and is executed with elevated privileges. Commands in 
# %setup can alter and potentially damage the host.
	
    touch README.md
    touch ${SINGULARITY_ROOTFS}/TEST.md

%files

# The %files section allows you to copy files into the container with greater safety 
# than using the %setup section. Files in the %files section are always copied before 
# the %post section is executed so that they are available during the build and 
# configuration process. Its general form is: <source> [<destination>]
	
    README.md
    

%post

# This section is where you can download files from the internet with tools like git and wget, 
# install new software and libraries, write configuration files, create new directories, etc.
# Let's start.
	
	# *** Environment Variables During Build
    export CUDNN_VERSION=7.6.5.32-1+cuda10.1		# 7.6.5.32-1+cuda10.1 (current), 7.5.1.10-1+cuda10.1
	export NCCL_VERSION=2.8.3-1+cuda10.1			# 2.8.3-1+cuda10.1 (current), 2.7.8-1+cuda10.1 (worked), 2.6.4-1+cuda10.1 (worked), 2.5.6-1+cuda10.1 (worked)
	export PYTHON_VERSION=3.8						# 3.8 (current)
    export TENSORFLOW_VERSION=2.3.0					# 2.3.0 (current)
    export PYTORCH_VERSION=1.6.0+cu101              # 1.7.0+cu101, 1.6.0+cu101 (current)
    export TORCHVISION_VERSION=0.7.0+cu101          # 0.8.1+cu101, 0.7.0+cu101 (current)
    
    # *** Build Container from Base Image
    
	# *** Add CUDA Repo. to install CUDA Toolkit 10.1 (skip, already installed.)
	# echo "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /" > /etc/apt/sources.list.d/cuda.list
	
	# *** Add Nividia Machine Learning Repo. to install CuDNN, NCCL2 etc.
	echo "deb https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /" > /etc/apt/sources.list.d/nvidia-ml.list
	
	# *** Update, Upgrade and Install Dependencies
	apt update && apt upgrade -y
	
	apt install -y \
		build-essential \
        cmake \
        git \
        curl \
        vim \
        wget \
        ca-certificates \
        libjpeg-dev \
        libpng-dev \
        python${PYTHON_VERSION} \
        python${PYTHON_VERSION}-dev \
        python3-pip
     
    # *** Install CuDNN, NCCL2 from Nividia Machine Learning Repo.
    # apt update && apt install -y libcudnn7=7.6.5.32-1+cuda10.1, libnccl2=2.8.3-1+cuda10.1 libnccl-dev=2.8.3-1+cuda10.1
    apt update && apt install -y --allow-downgrades --allow-change-held-packages --no-install-recommends \
        libcudnn7=${CUDNN_VERSION} \
        libnccl2=${NCCL_VERSION} \
        libnccl-dev=${NCCL_VERSION}
	
	# *** Hyperlink python3 as python
    ln -s /usr/bin/python${PYTHON_VERSION} /usr/bin/python
	
	# *** Install PIP3
	curl -O https://bootstrap.pypa.io/get-pip.py && \
    python get-pip.py && \
    rm get-pip.py
	
	# *** Install TensorFlow
    pip install tensorflow-gpu==${TENSORFLOW_VERSION} pandas jupyterlab tqdm h5py==2.10.0
    
    # *** Install PyTorch
	pip install torch==${PYTORCH_VERSION} torchvision==${TORCHVISION_VERSION} -f https://download.pytorch.org/whl/torch_stable.html
	
	# *** Install TrackML Library
    pip install 'trackml@https://github.com/LAL/trackml-library/tarball/master#egg=trackml-3'
    	
	# *** Install the IB verbs
    apt install -y --no-install-recommends libibverbs*
    apt install -y --no-install-recommends ibverbs-utils librdmacm* infiniband-diags libmlx4* libmlx5* libnuma*
	
	# *** Install Open MPI
    mkdir -p /tmp/openmpi && \
    cd /tmp/openmpi && \
    wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz && \
    tar zxf openmpi-4.0.0.tar.gz && \
    cd openmpi-4.0.0 && \
    ./configure --enable-orterun-prefix-by-default && \
    make -j $(nproc) all && \
    make install && \
    ldconfig && \
    rm -rf /tmp/openmpi
    cd /root
	
	# Install Horovod, temporarily using CUDA stubs
    ldconfig /usr/local/cuda-10.1/targets/x86_64-linux/lib/stubs && \
    HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITHOUT_PYTORCH=1 HOROVOD_WITHOUT_MXNET=1 pip install --no-cache-dir horovod && \
    ldconfig

	# Configure OpenMPI to run good defaults:
	#   --bind-to none --map-by slot --mca btl_tcp_if_exclude lo,docker0
    echo "hwloc_base_binding_policy = none" >> /usr/local/etc/openmpi-mca-params.conf && \
    echo "rmaps_base_mapping_policy = slot" >> /usr/local/etc/openmpi-mca-params.conf 
    #echo "btl_tcp_if_exclude = lo,docker0" >> /usr/local/etc/openmpi-mca-params.conf

	# Set default NCCL parameters
    echo NCCL_DEBUG=INFO >> /etc/nccl.conf && \
    echo NCCL_SOCKET_IFNAME=^docker0 >> /etc/nccl.conf

	# Download examples 
	cd / && \
	git clone https://github.com/horovod/horovod.git && \
	mv horovod/examples/ /examples && \
    rm -rf horovod

# -----------------------------------------------------------------------------
# 		Following Sections are Almost Same as horovod.def, exatrkx.def etc.
# ----------------------------------------------------------------------------- 

%test

# The %test section runs at the very end of the build process to validate the container 
# using a method of your choice. You can also execute this scriptlet through the container 
# itself, using the test command. For example, following scipt prints the base OS.
	
    grep -q NAME=\"Ubuntu\" /etc/os-release
    if [ $? -eq 0 ]; then
        echo "Container base is Ubuntu as expected."
    else
        echo "Container base is not Ubuntu."
    fi


%environment

# The %environment section allows you to define environment variables that will be set 
# at runtime. Note that these variables are not made available at build time by their 
# inclusion in the %environment section. This means that if you need the same variables 
# during the build process, you should also define them in your %post section. Specifically:
#
# (*) during build: The %environment section is written to a file in the container 
#     metadata directory. This file is not sourced.
# (*) during runtime: The file in the container metadata directory is sourced.
#
# You should use the same conventions that you would use in a .bashrc or .profile file.

	export LC_ALL=C
	export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

	export HOROVOD_GPU_ALLREDUCE=NCCL
	export HOROVOD_GPU_ALLGATHER=MPI
	export HOROVOD_GPU_BROADCAST=MPI

	# NCCL2: From Local NCCL Repo. such as nccl-repo-<version>.deb OR nccl-repo-<version>.rpm OR nccl-<version>.txz
	# (*) sudo dpkg -i nccl-repo-<version>.deb
	# (*) export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nccl-<version>/lib
	# (*) export HOROVOD_NCCL_HOME=/usr/local/nccl-<version>
	# 	  export HOROVOD_NCCL_INCLUDE=/usr/local/nccl-<version>/include
	#     export HOROVOD_NCCL_LIB=/usr/local/nccl-<version>/lib

	# NCCL2: For Network Repo. such as nvidia-machine-learning-repo-<version>.deb
	# (*) sudo dpkg -i nvidia-machine-learning-repo-<version>.deb
	#     export HOROVOD_NCCL_HOME (don't export)
	# (*) However, if really needed then point to 
	#     export HOROVOD_NCCL_HOME=/usr/lib/x86_64-linux-gnu

	# We Don't NEED THEM
	# export CUDNN_VERSION=7.6.5.32-1+cuda10.1        # 7.6.5.32-1+cuda10.1 (current), 7.5.1.10-1+cuda10.1
	# export NCCL_VERSION=2.8.3-1+cuda10.1            # 2.8.3-1+cuda10.1 (current), 2.7.8-1+cuda10.1 (worked), 2.6.4-1+cuda10.1 (worked), 2.5.6-1+cuda10.1 (worked)
	# export PYTHON_VERSION=3.8                       # 3.8 (current)
	# export TENSORFLOW_VERSION=2.3.0                 # 2.3.0 (current)
	# export PYTORCH_VERSION=1.6.0+cu101              # 1.7.0+cu101, 1.6.0+cu101 (current)
	# export TORCHVISION_VERSION=0.7.0+cu101          # 0.8.1+cu101, 0.7.0+cu101 (current)


%startscript

# Similar to the %runscript section, the contents of the %startscript section are
# written to a file within the container at build time. This file is executed when
# the instance start command is issued. For example
	
	echo "The Start Script is Empty."


%runscript

# The contents of the %runscript section are written to a file within the container 
# that is executed when the container image is run (either via the singularity run 
# command or by executing the container directly as a command). When the container 
# is invoked, arguments following the container name are passed to the runscript. 
# This means that you can (and should) process arguments within your runscript.
	
    echo "Container was created $NOW"
    echo "Arguments received: $*"
    exec echo "$@"

 
%labels

# The %labels section is used to add metadata to the file /.singularity.d/labels.json 
# within your container. The general format is a name-value pair.
	
    Author Adeel Akram
    Email adeel.akram@physics.uu.se


%help
    
    This Singularity definition contains Ubuntu 18.04, TensorFlow 2.3.0, PyTorch 1.7 w/ CUDATOOLKIT, CuDNN, NCCL2 etc.
    Moreover, everything is installed in the system e.g. python3, pip3, tensorflow, pytorch, openmpi and horovod.
    
    Note that, base Ubuntu 18.04 docker image already have CUDA 10.1 pre-installed.
    
    
